# AI Context: Transfer Learning BCI EEG Classification

> This file is written for AI assistants. It explains what this codebase does,
> how it is structured, what each component is responsible for, and what state
> the project is currently in. Read this before touching any code.

---

## What This Project Is

A **thesis research project** implementing a dual-branch deep learning architecture
for Motor Imagery (MI) EEG brain-computer interface (BCI) classification. The
central research question is:

> Can transfer learning from a large, weakly-labelled EEG dataset improve
> Left vs. Right Hand Motor Imagery classification accuracy when only a small
> amount of labelled target data is available?

The **primary dataset** is BCI Competition IV-2a (9 subjects, 22 EEG channels,
binary left/right hand MI task, ~144 trials per subject). The secondary dataset
is PhysioNet MMIDB (109 subjects), used as a pretraining source.

---

## Architecture

```
Input EEG epoch  (n_trials, n_channels, n_times)
         │
         ├─────────────────────────────────────┐
         │                                     │
    Branch A: ViT                        Branch B: Math
    ─────────────────                    ─────────────────
    CWT Morlet spectrogram               CSP (6 components,
    C3→R, Cz→G, C4→B                    Ledoit-Wolf reg)
    → (n_trials, 3, 224, 224)            +
    → ViT-Tiny (timm)                    Riemannian tangent
      pretrained ImageNet                space features (253-dim
    → 192-dim feature vector             for 22 channels)
                                         → concat → 259-dim
                                         → MLP(259→256→128)
                                         → 128-dim feature vector
         │                                     │
         └──────────────┬──────────────────────┘
                        │
               AttentionFusion(192, 128)
                        │
                   128-dim vector
                        │
               ClassifierHead(128→64→2)
                        │
               Softmax → Left / Right
```

**Total parameters: ~5.71M** (dominated by ViT-Tiny backbone ~5.5M).

ViT-Tiny (`vit_tiny_patch16_224`) was chosen for CPU/Colab compatibility — it
is the smallest model in the timm ViT family.

---

## Datasets

### BCI Competition IV-2a (primary target)
- Downloaded to: `~/mne_data/MNE-bnci-data/database/data-sets/001-2014/`
- Files: `A01T.mat` … `A09T.mat` (training) + `A01E.mat` … `A09E.mat` (evaluation)
- Access via MOABB: `BNCI2014_001` + `LeftRightImagery` paradigm
- 9 subjects, 22 channels, 250 Hz native → resampled to 128 Hz by MOABB
- After resampling: (n_trials, 22, 512) per subject, ~144 trials per subject

### PhysioNet MMIDB (pretraining source)
- Access via MOABB: `PhysionetMI` + `LeftRightImagery` paradigm
- 109 subjects, 64 channels
- Used only to pretrain the ViT branch on a larger, noisier EEG corpus

### Synthetic data (development/testing)
- Generated by `make_synthetic_subject_data()` in `src/bci/training/cross_validation.py`
- Produces (n_subjects × 144 trials, 22 channels, 512 timepoints)
- Contains a 10 Hz mu-rhythm signal on channels 3/4 that discriminates classes
- Used in all unit tests (no download required) and for quick smoke-tests

---

## Project Structure

```
bci_code/
├── CONTEXT.md                    ← you are here
├── README.md                     ← full setup and run instructions
├── pyproject.toml                ← Python 3.13, uv, all dependencies
├── PHASE_0_REPORT.md … PHASE_4_REPORT.md   ← thesis progress reports
│
├── src/bci/                      ← installable library package
│   ├── data/
│   │   ├── download.py           ← MOABB dataset download + loading utilities
│   │   ├── preprocessing.py      ← EEG preprocessing pipeline (filter, ICA, epoch)
│   │   ├── transforms.py         ← CWT Morlet spectrogram generation (pywt + PIL)
│   │   ├── dataset.py            ← PyTorch Dataset classes (SpectrogramDataset,
│   │   │                           EEGFeatureDataset, DualBranchDataset)
│   │   └── dual_branch_builder.py← Per-fold CSP+Riemannian fit + CWT image generation
│   │                               Returns TensorDataset(image, features, label)
│   ├── features/
│   │   ├── csp.py                ← CSP spatial filter wrapper (MNE, sklearn API)
│   │   └── riemannian.py         ← Riemannian tangent space wrapper (pyriemann, sklearn API)
│   ├── models/
│   │   ├── vit_branch.py         ← ViT-Tiny from timm as feature extractor or classifier
│   │   ├── math_branch.py        ← MLP for handcrafted features
│   │   ├── fusion.py             ← ConcatFusion, AttentionFusion, GatedFusion
│   │   └── dual_branch.py        ← Full DualBranchModel assembling all components
│   ├── training/
│   │   ├── trainer.py            ← Generic PyTorch Trainer (AdamW, cosine LR, early stop)
│   │   ├── cross_validation.py   ← within_subject_cv, loso_cv, make_synthetic_subject_data
│   │   └── evaluation.py         ← compute_metrics (accuracy, kappa, F1, AUC)
│   └── utils/
│       ├── config.py             ← Dataclass configs (ModelConfig, TrainingConfig, etc.)
│       ├── seed.py               ← set_seed(), get_device()
│       └── visualization.py      ← Matplotlib helpers for training curves, confusion matrices
│
├── scripts/                      ← Runnable experiment scripts (not importable as library)
│   ├── baseline_a_csp_lda.py     ← Phase 1: CSP + LDA baseline
│   ├── baseline_b_riemannian.py  ← Phase 1: Riemannian + LDA baseline
│   ├── baseline_c_vit.py         ← Phase 1: ViT-only baseline (standalone, no math branch)
│   ├── train_dual_branch.py      ← Phase 2: Full dual-branch model training
│   ├── pretrain_physionet.py     ← Phase 3 step 1: Pretrain ViT on PhysioNet source data
│   ├── finetune_bci_iv2a.py      ← Phase 3 step 2: Compare scratch/imagenet/transfer
│   ├── reduced_data_experiment.py← Phase 3 step 3: Core thesis – accuracy vs data fraction
│   ├── phase4_compile_results.py ← Phase 4: Read all JSON results, print summary table
│   ├── phase4_visualize.py       ← Phase 4: Generate thesis figures (matplotlib)
│   ├── phase4_stats.py           ← Phase 4: Wilcoxon, t-test, Friedman, Cohen's d
│   └── smoke_test.py             ← Quick sanity check without pytest
│
├── tests/
│   ├── test_phase1.py            ← 18 tests (CSP, Riemannian, LDA baselines, CV)
│   ├── test_phase2.py            ← 17 tests (dual-branch model, fusion, trainer)
│   └── test_phase3.py            ← 14 tests (pretrain, transfer, reduced-data)
│
├── results/                      ← JSON output from experiment scripts
│   ├── real_baseline_a_csp_lda.json     ← REAL DATA: CSP+LDA (79.32% within, 65.78% LOSO)
│   ├── baseline_a_csp_lda.json          ← duplicate of above (kept for reference)
│   ├── real_baseline_b_riemannian.json  ← REAL DATA: Riemannian (61.65% within, 63.85% LOSO)
│   ├── real_baseline_c_vit.json         ← REAL DATA: ViT-only (52.58% within ±8.1%, near chance)
│   ├── baseline_b_riemannian.json       ← SYNTHETIC: Riemannian (100% — 3 subjects, ignore)
│   ├── baseline_c_vit.json              ← SYNTHETIC: ViT-only (ignore)
│   ├── dual_branch_*.json               ← SYNTHETIC dual-branch results (not real)
│   ├── finetune_*.json                  ← SYNTHETIC finetune results (not real)
│   ├── reduced_data_results.json        ← SYNTHETIC reduced-data results (not real)
│   └── phase4_summary.json              ← Compiled table: real baselines + synthetic others
│
├── checkpoints/
│   └── vit_pretrained_physionet.pt  ← ViT weights pretrained on synthetic PhysioNet
│
├── configs/
│   ├── default.yaml              ← Base experiment config
│   └── dataset/bci_iv2a.yaml     ← Dataset-specific overrides
│
└── figures/                      ← (created by phase4_visualize.py)
```

---

## Key Design Decisions

### No Data Leakage in CV
`DualBranchFoldBuilder.build_fold()` fits CSP and Riemannian estimators **only**
on the training split of each fold. The test split is transformed using the
already-fitted estimators. This is enforced inside `build_fold()` before any
data touches the test set.

### CWT Spectrogram Format
EEG epochs → Continuous Wavelet Transform (Morlet, 4–40 Hz, 64 frequency bins) →
magnitude spectrogram → min-max normalize to uint8 → resize to 224×224 via PIL.
Three channels: C3 → Red, Cz → Green, C4 → Blue. This encodes the laterality
of motor cortex activity as color balance in the image.

### MOABB Resampling
All scripts that feed data to the ViT (via CWT spectrograms) call MOABB's
`LeftRightImagery(fmin=4.0, fmax=40.0, resample=128.0)` to get 512 timepoints
at 128 Hz. This is required for consistent CWT scale computation. The
`sfreq=128.0` is hardcoded in `DualBranchFoldBuilder`.

### Math Branch Feature Dimensions
For 22-channel BCI IV-2a data:
- CSP: 6 components → 6-dim vector
- Riemannian: 22 × 23 / 2 = 253-dim tangent space vector
- Concatenated: 259-dim total (`math_input_dim = 259`)

For 3-channel synthetic/testing data:
- CSP: 6 → 6-dim
- Riemannian: 3 × 4 / 2 = 6-dim
- Concatenated: 12-dim total

### Fusion Methods (Ablation Study)
Three fusion variants are implemented and tested:
- `concat` — simple concatenation + linear projection
- `attention` — learned attention weights over both branches (default, thesis target)
- `gated` — sigmoid gate controlling branch contributions

The `--fusion` flag in `train_dual_branch.py` selects which to use.

### BatchNorm with Small Batches
`Trainer._split_dataset()` sets `drop_last=n_train > batch_size` to prevent
BatchNorm from receiving a batch of size 1 (which causes a NaN crash). This
matters when training with very small datasets (reduced-data experiments).

### Transfer Learning Pipeline
```
Phase 3 Step 1: pretrain_physionet.py
    → Trains ViT branch standalone (as classifier) on PhysioNet source data
    → Saves ViT weights to checkpoints/vit_pretrained_eeg.pt

Phase 3 Step 2: finetune_bci_iv2a.py
    → Loads checkpoint into DualBranchModel.vit_branch
    → Freezes ViT backbone (keeps last 2 blocks + head trainable)
    → Fine-tunes full DualBranchModel on BCI IV-2a target data
    → Compares: scratch | imagenet | transfer conditions

Phase 3 Step 3: reduced_data_experiment.py
    → For each fraction in [0.10, 0.25, 0.50, 0.75, 1.00]:
        Subsamples training data
        Trains scratch vs. transfer DualBranchModel
        Reports accuracy ± std (n_repeats random seeds)
```

---

## Configuration System

Dataclasses in `src/bci/utils/config.py` are the single source of truth for
all hyperparameters. They can be instantiated with defaults or overridden from
YAML files via `load_config(path)`.

Key configs used at runtime:
- `ModelConfig` — ViT model name, drop rates, fusion method, classifier dims
- `SpectrogramConfig` — wavelet, frequency range, image size, channel mode
- `TrainingConfig` — LR, weight decay, epochs, patience, seed, device

The `Trainer` class ignores `TrainingConfig` entirely (it takes kwargs directly).
`ModelConfig` is the most actively used at runtime.

---

## Experiment Status

| Experiment | Data | Status |
|---|---|---|
| Baseline A: CSP + LDA | Real BCI IV-2a | **Done** — 79.32% within / 65.78% LOSO |
| Baseline B: Riemannian + LDA | Real BCI IV-2a | **Done** — 61.65% within / 63.85% LOSO |
| Baseline C: ViT-only | Real BCI IV-2a | **Done** — 52.58% within (near chance) |
| Dual-branch (all fusion methods) | Synthetic only | Needs GPU re-run with real data |
| Phase 3: Pretrain ViT | Synthetic source | Checkpoint exists (`vit_pretrained_physionet.pt`) |
| Phase 3: Finetune comparison | Synthetic only | Needs GPU re-run with real data |
| Phase 3: Reduced-data | Synthetic only | Needs GPU re-run with real data |
| Phase 4: Analysis + figures | Mixed | Scripts ready, awaiting GPU results |

The Baseline C ViT-only result (52.58%, near chance) is expected and motivates
the dual-branch approach. ViT alone on EEG spectrograms is insufficient with
this dataset size. The dual-branch + transfer learning combination is the thesis
contribution.

---

## How to Run

```bash
# Install dependencies
uv sync

# Run all tests (uses synthetic data, no GPU needed)
uv run pytest tests/ -v

# Run a single baseline (real data must be downloaded)
uv run python scripts/baseline_a_csp_lda.py --data real

# Run dual-branch training (requires GPU for reasonable speed)
uv run python scripts/train_dual_branch.py --data real --n-folds 5 --epochs 50 \
    --fusion attention --output results/real_dual_branch_attention.json

# Full Phase 3 pipeline (GPU)
uv run python scripts/pretrain_physionet.py --data synthetic --epochs 50 \
    --checkpoint checkpoints/vit_pretrained_eeg.pt
uv run python scripts/finetune_bci_iv2a.py --data real \
    --checkpoint checkpoints/vit_pretrained_eeg.pt \
    --conditions scratch imagenet transfer --n-folds 5 --epochs 50
uv run python scripts/reduced_data_experiment.py --data real \
    --checkpoint checkpoints/vit_pretrained_eeg.pt \
    --fractions 0.10 0.25 0.50 0.75 1.00 --n-folds 5 --n-repeats 3 --epochs 50

# Phase 4: Compile + visualize + stats (after GPU runs)
uv run python scripts/phase4_compile_results.py --prefix real_
uv run python scripts/phase4_visualize.py --summary results/phase4_summary.json
uv run python scripts/phase4_stats.py --summary results/phase4_summary.json
```

---

## Environment Notes

- **Python 3.13**, managed with **uv** (`uv run` to use the venv)
- **PyTorch CPU-only** by default (pinned to pytorch-cpu index in pyproject.toml)
- **For GPU training**: reinstall torch with CUDA, e.g.
  `uv pip install torch --index-url https://download.pytorch.org/whl/cu121`
- The `.venv` is NOT indexed by the LSP. All import errors shown by the editor
  are **false positives**. Imports work correctly at runtime.
- `get_device()` in `seed.py` auto-selects CUDA > MPS > CPU.
- `pyproject.toml` sets `pythonpath = ["src", "scripts"]` so `from bci.x import y`
  and `from scripts.x import y` both work from the project root.

---

## Known Remaining Issues (Non-Critical)

These were identified during cleanup but not fixed because they would require
larger refactoring with no immediate benefit given the thesis timeline:

1. **Duplicated `load_real_data()` function** — appears in `baseline_a`, `baseline_b`,
   `baseline_c`, `train_dual_branch`, `finetune_bci_iv2a`, `reduced_data_experiment`.
   Each version is nearly identical (MOABB `LeftRightImagery` + subject loop).
   Consolidating into `bci.data.download` would be the right fix.

2. **Duplicated `_epochs_to_images()` helper** — appears in `baseline_c_vit.py`,
   `pretrain_physionet.py`, and `dual_branch_builder.py`. All three do the same
   HWC→CHW + /255.0 conversion. The builder version is the canonical one.

3. **Duplicated `_build_model()` function** — identical in `finetune_bci_iv2a.py`
   and `reduced_data_experiment.py`. Should be extracted to a shared helper.

4. **`CVResult.summary()`** is never called by any script. Scripts print their own
   tables. The method is correct and useful but orphaned.

5. **`print_classification_report()`** in `evaluation.py` is never called anywhere.

6. **`visualization.py` utilities** are never imported by `phase4_visualize.py`,
   which uses matplotlib directly. The visualization module is effectively unused.

7. **`compute_covariances()` static method** in `riemannian.py` is never called.
   It is documented as a utility for visualization, but no visualization code uses it.

8. **`get_common_channels()`** in `download.py` downloads real data just to inspect
   channel names — a side-effect that can accidentally trigger a dataset download.
